{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checker File #\n",
    "    To check the modules built in the source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import source\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000,-1)/255.0\n",
    "X_test = X_test.reshape(10000,-1)/255.0\n",
    "y_train = source.one_hot_numpy(y_train)\n",
    "y_test = source.one_hot_numpy(y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Tensorflow model...\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.activations import linear,sigmoid,relu\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(784,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with sparse CE\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, validation_data = (X_test,y_test), batch_size = 100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Tensorflow model...\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.activations import linear,sigmoid,relu\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Model\n",
    "#tf.random.set_seed(1234) # for consistent results\n",
    "model_NN = Sequential(\n",
    "    [               \n",
    "        tf.keras.Input(shape=(784,)), #input the size\n",
    "        Dense(units=512 , activation=relu),\n",
    "        Dense(units=512 , activation=relu),\n",
    "        Dense(units=10 , activation=linear),\n",
    "    ], name = \"my_model\" \n",
    ")\n",
    "\n",
    "boundaries = [3000, 5000]  # The steps at which the learning rate will change\n",
    "values = [0.001, 0.0005, 0.0001]  # Learning rates for each interval\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=boundaries,\n",
    "    values=values\n",
    ")\n",
    "\n",
    "#[layer1, layer2, layer3] = model_NN.layers\n",
    "model_NN.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model_NN.fit(  # Define X_train, y_train, X_cv, y_cv\n",
    "    X_train,y_train,\n",
    "    epochs=10, ## Early stopped\n",
    "    batch_size = 100,\n",
    "    validation_data = (X_test,y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = [784, 64,10]\n",
    "activation_sequence =   ['relu', 'softmax']\n",
    "optimizer = \"adam\"\n",
    "learning_rate = 1e-2\n",
    "loss = \"cross_entropy\"\n",
    "initialization = \"Xavier\"\n",
    "momentum = 0.95\n",
    "weight_decay = 0.0001\n",
    "beta_rms = 0.95\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.99\n",
    "md2 = source.FeedForwardNeuralNetwork(arch=arch, activation_sequence=activation_sequence, optimizer=optimizer,\n",
    "                                      learning_rate=learning_rate,weight_decay= weight_decay, loss=loss,initialization=initialization,momentum=momentum,beta_rms=beta_rms,\n",
    "                                      beta_1=beta_1,beta_2=beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = source.Batchloader(X_train, y_train, batch_size=64,shuffle=True)\n",
    "batch_test = source.Batchloader(X_test, y_test, batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "Epoch 1 || loss_train : 0.40085881901698195 | loss_test : 0.44215337337595284 || acc_train : 0.8575166666666667 | acc_test : 0.8447 | acc_test_l : 0.8447|\n",
      "Epoch : 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#if i%(600/4) == 0:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#    print(f\" part {int(i//(600/4))}/4 done !!!\")\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# One step training\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mmd2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m md2\u001b[38;5;241m.\u001b[39mforward_call(inputs_\u001b[38;5;241m=\u001b[39mX_train)\n\u001b[0;32m     15\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m md2\u001b[38;5;241m.\u001b[39mforward_call(inputs_\u001b[38;5;241m=\u001b[39mX_test)\n",
      "File \u001b[1;32mc:\\Users\\Joeld\\Desktop\\IITM\\SEM-08\\DA6401 - DEEP LEARNING\\DA6401-A01\\source.py:738\u001b[0m, in \u001b[0;36mFeedForwardNeuralNetwork.train_step\u001b[1;34m(self, X_train, y_train, epoch)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03mDoes one step of training of the model\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# Forward propagation\u001b[39;00m\n\u001b[1;32m--> 738\u001b[0m _, preac, postac \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_sequence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_seqence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;66;03m# Do one optimizer step\u001b[39;00m\n\u001b[0;32m    740\u001b[0m Weights, Biases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mOptimizer_class\u001b[38;5;241m.\u001b[39mstepper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases, preac, postac, y_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_seqence, epoch\u001b[38;5;241m=\u001b[39mepoch)\n",
      "File \u001b[1;32mc:\\Users\\Joeld\\Desktop\\IITM\\SEM-08\\DA6401 - DEEP LEARNING\\DA6401-A01\\source.py:208\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(input_, Weights, Biases, activation_sequence)\u001b[0m\n\u001b[0;32m    206\u001b[0m activation \u001b[38;5;241m=\u001b[39m activation_sequence[i]\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# computing pre activation\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m pre_ac \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_reshaped\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# (DEBUG) print(input_reshaped.min(), input_reshaped.max(), W.min(), W.max(), b.min(), b.max())\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# appending to the pre activation matrix\u001b[39;00m\n\u001b[0;32m    211\u001b[0m fp_pre_ac\u001b[38;5;241m.\u001b[39mappend(pre_ac)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "epochs = 5\n",
    "#accuracy_train, accuracy_cv, loss_train, loss_cv = np.array([]), np.array([]), np.array([]), np.array([])\n",
    "for epoch in range(1,epochs+1):\n",
    "    i = 0\n",
    "    print(f\"Epoch : {epoch}\")\n",
    "    for X_, y_ in batch_train:\n",
    "        i += 1\n",
    "        #if i%(600/4) == 0:\n",
    "        #    print(f\" part {int(i//(600/4))}/4 done !!!\")\n",
    "        # One step training\n",
    "        md2.train_step(X_, y_, epoch)\n",
    "\n",
    "    train_pred = md2.forward_call(inputs_=X_train)\n",
    "    test_pred = md2.forward_call(inputs_=X_test)\n",
    "\n",
    "    loss_train = source.find_loss(y_pred= train_pred, y_true =y_train, loss=\"cross_entropy\")\n",
    "    loss_test = source.find_loss(y_pred= test_pred, y_true = y_test, loss = \"cross_entropy\")\n",
    "\n",
    "    train_pred = md2.forward_call(inputs_=X_train,threshold=True)\n",
    "    test_pred = md2.forward_call(inputs_=X_test,threshold=True)\n",
    "    test_pred_l= md2.forward_call(inputs_=X_test,with_logits=True,threshold=True)\n",
    "\n",
    "    accuracy_train = source.accuracy(train_pred, y_train)\n",
    "    accuracy_test = source.accuracy(test_pred, y_test)\n",
    "    accuracy_test_l = source.accuracy(test_pred_l, y_test)\n",
    "    if epoch%1 == 0:\n",
    "        print(F\"Epoch {epoch} || loss_train : {loss_train} | loss_test : {loss_test} || acc_train : {accuracy_train} | acc_test : {accuracy_test} | acc_test_l : {accuracy_test_l}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred1 = md1.forward_call(inputs_=batch_train, is_batch_both=True)\n",
    "#train_pred2 = md1.forward_call(inputs_=X_train,threshold=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 1.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000],\n",
       "       [1.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 1.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000],\n",
       "       [0.00000000e+000, 1.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 3.39040216e-175, 0.00000000e+000,\n",
       "        0.00000000e+000, 1.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000]], dtype=float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([inf])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([1e1000]).astype(np.longdouble)\n",
    "A / 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_pred1[:5],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.71828183e+00 2.71828183e+00 2.00855369e+01]\n",
      " [5.45981500e+01 1.48413159e+02 2.20264658e+04]]\n",
      "[[   57.31643186   151.13144093 22046.55133173]]\n",
      "[[4.74258732e-02 1.79862100e-02 9.11051194e-04]\n",
      " [9.52574127e-01 9.82013790e-01 9.99088949e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.74258732e-02, 1.79862100e-02, 9.11051194e-04],\n",
       "       [9.52574127e-01, 9.82013790e-01, 9.99088949e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,1,3],[4,5,10]])\n",
    "B = np.sum(A,axis = 0, keepdims=True)\n",
    "\n",
    "source.softmax(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = [784, 128, 32, 64, 10] \n",
    "activation_sequence = ['relu', 'relu', 'sigmoid', 'softmax']\n",
    "optimizer = \"sgd\"\n",
    "learning_rate = 0.001\n",
    "loss = \"cross_entropy\"\n",
    "initialization = \"random\"\n",
    "momentum = 0\n",
    "md1 = source.FeedForwardNeuralNetwork(arch=arch, activation_sequence=activation_sequence, optimizer=optimizer,\n",
    "                                      learning_rate=learning_rate, loss=loss,initialization=initialization,momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the transformation #\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converting PIL image to a PyTorch tensor #\n",
    "])\n",
    "\n",
    "# Importing the dataset #\n",
    "train_dataset = torchvision.datasets.MNIST(root = \"C:/Users/Joeld/Desktop/IITM/SEM-08/\" , train= True, transform=transform, download= True)\n",
    "# Loading the test data #\n",
    "test_dataset = torchvision.datasets.MNIST(root = \"C:/Users/Joeld/Desktop/IITM/SEM-08/\", train= False, transform=transform , download= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_dataset.data.numpy()\n",
    "train_X = train_X.reshape(60000,-1)\n",
    "train_X = train_X.astype(np.longdouble)\n",
    "train_y = source.one_hot_numpy(train_dataset.targets.numpy())\n",
    "\n",
    "test_X = test_dataset.data.numpy()\n",
    "test_X = test_X.reshape(10000,-1)\n",
    "test_X = test_X.astype(np.longdouble)\n",
    "test_y = source.one_hot_numpy(test_dataset.targets.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=int64),\n",
       " tensor([7, 2, 1, 0, 4]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[:5], test_dataset.targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = source.Batchloader(train_X, train_y, batch_size=5000,shuffle=False)\n",
    "batch_test = source.Batchloader(test_X, test_y, batch_size=5000,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_train, y_train \u001b[38;5;129;01min\u001b[39;00m batch_train:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# One step training\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mmd1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m md1\u001b[38;5;241m.\u001b[39mforward_call(inputs_\u001b[38;5;241m=\u001b[39mbatch_train,is_batch_both\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m md1\u001b[38;5;241m.\u001b[39mforward_call(inputs_\u001b[38;5;241m=\u001b[39mbatch_test,is_batch_both\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Joeld\\Desktop\\IITM\\SEM-08\\DA6401 - DEEP LEARNING\\DA6401-A01\\source.py:482\u001b[0m, in \u001b[0;36mFeedForwardNeuralNetwork.train_step\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m    480\u001b[0m _, preac, postac \u001b[38;5;241m=\u001b[39m forward_propagation(X_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases, activation_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_seqence)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Do one optimizer step\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m Weights, Biases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptimizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstepper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_seqence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Update the weights\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_params(Weights, Biases)\n",
      "File \u001b[1;32mc:\\Users\\Joeld\\Desktop\\IITM\\SEM-08\\DA6401 - DEEP LEARNING\\DA6401-A01\\source.py:427\u001b[0m, in \u001b[0;36mOptimizer.stepper\u001b[1;34m(self, Weights, Biases, pre_ac, post_ac, y_true, activation_sequence)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msgd_step(Weights, Biases, pre_ac, post_ac, y_true, activation_sequence)\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgd_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWeights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_ac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_ac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_sequence\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joeld\\Desktop\\IITM\\SEM-08\\DA6401 - DEEP LEARNING\\DA6401-A01\\source.py:381\u001b[0m, in \u001b[0;36mOptimizer.gd_step\u001b[1;34m(self, Weights, Biases, pre_ac, post_ac, y_true, activation_sequence)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgd_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, Weights, Biases, pre_ac, post_ac, y_true, activation_sequence):\n\u001b[0;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    Does one step gradient descent of weights (does in place)\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    Input:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m    Biases : list of bias matrices list[<numpy.ndarray>]\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 381\u001b[0m     grads_wrt_weights, grads_wrt_biases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWeights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_ac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_ac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Weights)):\n\u001b[0;32m    383\u001b[0m         Weights[i] \u001b[38;5;241m=\u001b[39m Weights[i] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\u001b[38;5;241m*\u001b[39mgrads_wrt_weights[\u001b[38;5;241m-\u001b[39mi\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Joeld\\Desktop\\IITM\\SEM-08\\DA6401 - DEEP LEARNING\\DA6401-A01\\source.py:341\u001b[0m, in \u001b[0;36mOptimizer.backprop_grads\u001b[1;34m(self, Weights, Biases, pre_ac, post_ac, y_true, activation_sequence)\u001b[0m\n\u001b[0;32m    338\u001b[0m b \u001b[38;5;241m=\u001b[39m Biases[\u001b[38;5;241m-\u001b[39mlayer]\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Finding gradients with respect to weights and biases (average along batch size)\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# (- checker ) print(f\" Layer : {-layer}  \\n W : {W.shape} \\n B : {b.shape} \\n preac : {grads_wrt_preac.shape}\")\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m grads_W \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mij,kj->ikj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mgrads_wrt_preac\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_\u001b[49m\u001b[43m)\u001b[49m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    342\u001b[0m grads_b \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(grads_wrt_preac, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# check the shapes of the gradient matches with the matrix size\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joeld\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\einsumfunc.py:1371\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[0;32m   1370\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m-> 1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "epochs = 10\n",
    "#accuracy_train, accuracy_cv, loss_train, loss_cv = np.array([]), np.array([]), np.array([]), np.array([])\n",
    "for epoch in range(1,epochs+1):\n",
    "    i = 1\n",
    "    for X_train, y_train in batch_train:\n",
    "        # One step training\n",
    "        md1.train_step(X_train, y_train)\n",
    "\n",
    "    train_pred = md1.forward_call(inputs_=batch_train,is_batch_both=True)\n",
    "    test_pred = md1.forward_call(inputs_=batch_test,is_batch_both=True)\n",
    "\n",
    "    accuracy_train = precision_score(y_pred= train_pred, y_true = train_y, average=\"weighted\")\n",
    "    accuracy_test = precision_score(y_pred= test_pred, y_true = test_y, average=\"weighted\")\n",
    "\n",
    "    print(F\"Epoch {epoch} acc_train : {accuracy_train} acc_test : {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = md1.forward_call(batch_test, is_batch_both = True)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10218978, 0.11645299, 0.09763948, 0.09854423, 0.09080717,\n",
       "       0.08539326, 0.09711846, 0.09655938, 0.10509554, 0.11476664])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_acc = precision_score(y_pred= y_pred, y_true = test_y, average=None)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import source\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Info = [5, 5, 3]\n",
    "Weights, Biases = source.init_mat(Info, init_scheme=\"Xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 10), (20, 20), (2, 20), (20, 1), (20, 1), (2, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weights[0].shape, Weights[1].shape, Weights[2].shape, Biases[0].shape, Biases[1].shape, Biases[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random(size = (8,10))\n",
    "B = np.array([[0,1],[1,0],[1,0],[0,1],[1,0],[0,1],[0,1],[1,0]])\n",
    "\n",
    "act = [\"tanh\",\"relu\",\"softmax\"]\n",
    "output,preac,postac = source.forward_propagation(input_=A,Weights=Weights,Biases=Biases,activation_sequence=act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.48083072, 0.57882088, 0.22890125, 0.69667756, 0.48330189,\n",
       "         0.89149734, 0.05391035, 0.20764917],\n",
       "        [0.43618723, 0.09015036, 0.08780259, 0.99657987, 0.00278176,\n",
       "         0.34594457, 0.94818285, 0.18632982],\n",
       "        [0.25743761, 0.57585937, 0.65681746, 0.75881264, 0.81363467,\n",
       "         0.54674836, 0.95280503, 0.92658912],\n",
       "        [0.52929924, 0.55846963, 0.03380309, 0.20248469, 0.61217501,\n",
       "         0.94150741, 0.89508066, 0.89435619],\n",
       "        [0.02288824, 0.32877616, 0.57315616, 0.51274884, 0.43955063,\n",
       "         0.06529684, 0.64015198, 0.43741116],\n",
       "        [0.67857216, 0.55982527, 0.16730233, 0.09865947, 0.49268503,\n",
       "         0.34374155, 0.28867639, 0.87369352],\n",
       "        [0.9309412 , 0.68630334, 0.0883853 , 0.75790219, 0.13193485,\n",
       "         0.4501492 , 0.66104717, 0.92272033],\n",
       "        [0.16306415, 0.02396907, 0.83487185, 0.53280878, 0.78235778,\n",
       "         0.66840801, 0.45208439, 0.29161784],\n",
       "        [0.07698133, 0.10959643, 0.60176329, 0.48989938, 0.65147018,\n",
       "         0.30773208, 0.32371961, 0.44046983],\n",
       "        [0.56515039, 0.99062612, 0.20573685, 0.44518589, 0.21636013,\n",
       "         0.23247247, 0.83194723, 0.42131401]]),\n",
       " array([[ 0.61703502,  0.69514159,  0.07468446,  0.22079312,  0.45496346,\n",
       "          0.57238601,  0.60611249,  0.71235918],\n",
       "        [-0.14869186, -0.14864344, -0.24900918,  0.22614345, -0.47643204,\n",
       "         -0.39567483, -0.13764189, -0.50192062],\n",
       "        [ 0.57402437,  0.56582396, -0.00211176,  0.50576963, -0.05918014,\n",
       "          0.01456626,  0.71924676,  0.63749461],\n",
       "        [-0.37310308, -0.52643036,  0.10994936,  0.09288299, -0.12016679,\n",
       "         -0.23192438,  0.0382302 , -0.27243444],\n",
       "        [-0.38520897, -0.31603849, -0.42718758, -0.24921934, -0.3644246 ,\n",
       "         -0.24943245, -0.17257291, -0.51627459],\n",
       "        [ 0.05208424, -0.10447681, -0.47021388, -0.10258504, -0.48800721,\n",
       "         -0.23986876, -0.20778246, -0.27359496],\n",
       "        [ 0.23200896,  0.19521205,  0.67858908,  0.20199955,  0.76177628,\n",
       "          0.75023501,  0.30753312,  0.38410063],\n",
       "        [-0.28800808, -0.32161806, -0.16423571, -0.66145978, -0.1856833 ,\n",
       "         -0.33039635, -0.5605554 , -0.34037487],\n",
       "        [-0.75070418, -0.6824126 , -0.6476036 , -0.74502647, -0.69750704,\n",
       "         -0.76399171, -0.83065996, -0.76246249],\n",
       "        [-0.11832823, -0.09163983, -0.17755624, -0.15744544,  0.03860157,\n",
       "         -0.01754042,  0.35335254,  0.35145018],\n",
       "        [-0.08624709, -0.38120027, -0.51366413, -0.33526106, -0.39246113,\n",
       "         -0.1417303 , -0.2102559 , -0.13746063],\n",
       "        [ 0.56925244,  0.35868712, -0.04777543,  0.20638071,  0.28507762,\n",
       "          0.45917572,  0.00399786,  0.54669074],\n",
       "        [ 0.04668783,  0.21308902,  0.23228911,  0.18528807,  0.19977396,\n",
       "         -0.01625912,  0.36747522,  0.25928779],\n",
       "        [ 0.31050409,  0.23823076, -0.04042449,  0.10559543, -0.07629921,\n",
       "          0.01848977,  0.27659447,  0.11669025],\n",
       "        [ 0.17908374, -0.15598659, -0.00099148,  0.63746998, -0.35587395,\n",
       "         -0.22633433,  0.22270057, -0.09325636],\n",
       "        [ 0.5852146 ,  0.52444828,  0.19918849,  0.630579  ,  0.20483192,\n",
       "          0.44649223,  0.73723575,  0.43744781],\n",
       "        [-0.29088324, -0.26633578, -0.64675739, -0.65338565, -0.52477788,\n",
       "         -0.28824171, -0.63875323, -0.34308992],\n",
       "        [ 0.04659174, -0.16497036, -0.67567509, -0.58664978, -0.4391324 ,\n",
       "         -0.12980786, -0.02084638,  0.1594535 ],\n",
       "        [-0.57476937, -0.59604494, -0.1489634 , -0.59908168, -0.15196954,\n",
       "         -0.42384834, -0.6423136 , -0.42941032],\n",
       "        [ 0.00918946, -0.11312044, -0.07300171,  0.13480788,  0.04718558,\n",
       "          0.27315407,  0.23857331, -0.09879007]]),\n",
       " array([[0.80585901, 0.70388177, 1.05117692, 0.98740002, 0.90862301,\n",
       "         0.80235948, 0.77564246, 0.89102102],\n",
       "        [0.317067  , 0.33207552, 0.24078789, 0.47840697, 0.11611035,\n",
       "         0.07900267, 0.10600448, 0.16508301],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.29975841, 0.13531924, 0.        , 0.5839037 , 0.        ,\n",
       "         0.        , 0.03910759, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.04556114, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.13525257, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.14531395, 0.29338076, 0.24662745, 0.44427374, 0.16512206,\n",
       "         0.15586053, 0.4719953 , 0.23383211],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.88257696, 0.74418507, 0.1676236 , 0.57999238, 0.23221315,\n",
       "         0.57241901, 0.48411562, 0.55857276],\n",
       "        [0.62140952, 0.68384373, 0.8102585 , 0.55704323, 0.80492137,\n",
       "         0.74403304, 0.45077456, 0.58536347],\n",
       "        [0.        , 0.        , 0.20600619, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.03335372, 0.        , 0.18390623, 0.30956934, 0.        ,\n",
       "         0.        , 0.06664528, 0.        ],\n",
       "        [0.06704464, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.02422714, 0.        , 0.        ],\n",
       "        [0.        , 0.05929435, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.29101949, 0.07407433],\n",
       "        [0.        , 0.27429266, 0.49414084, 0.        , 0.61114315,\n",
       "         0.3242633 , 0.        , 0.05962433],\n",
       "        [0.1617911 , 0.21771504, 0.9402068 , 0.27132504, 0.7865276 ,\n",
       "         0.47392175, 0.02031084, 0.25956269],\n",
       "        [0.        , 0.        , 0.        , 0.00154675, 0.        ,\n",
       "         0.03605624, 0.        , 0.        ],\n",
       "        [0.31146886, 0.36029784, 0.10143068, 0.27920608, 0.20119419,\n",
       "         0.19046026, 0.36221626, 0.53406826],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]], dtype=float64),\n",
       " array([[0.59026454, 0.57290074, 0.41363561, 0.58552604, 0.47531242,\n",
       "         0.51863569, 0.50402583, 0.4683485 ],\n",
       "        [0.40973546, 0.42709926, 0.58636439, 0.41447396, 0.52468758,\n",
       "         0.48136431, 0.49597417, 0.5316515 ]], dtype=float64)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti = source.Optimizer(loss=\"cross_entropy\")\n",
    "gw,gb = opti.backprop_grads(Weights=Weights,Biases=Biases,pre_ac=preac,post_ac=postac,y_true=B,activation_sequence=act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.19984302,  0.3451834 ,  0.18167548],\n",
       "         [-0.55677034, -0.96169423, -0.50615486],\n",
       "         [ 0.35692732,  0.61651082,  0.32447938]], dtype=float64),\n",
       "  array([[-0.23490988, -0.46981976],\n",
       "         [-0.170014  , -0.34002801],\n",
       "         [ 0.24919027,  0.49838054]], dtype=float64)],\n",
       " [array([[ 0.18167548],\n",
       "         [-0.50615486],\n",
       "         [ 0.32447938]], dtype=float64),\n",
       "  array([[-0.23490988],\n",
       "         [-0.170014  ],\n",
       "         [ 0.24919027]], dtype=float64)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Info = [2,3,3]\n",
    "act = [\"relu\",\"softmax\"]\n",
    "A = np.array([[1,2]],dtype=np.longdouble)\n",
    "B = np.array([[0,1,0]],dtype=np.longdouble)\n",
    "Weights = (np.array([[0.2,0.4],[0.3,0.7],[0.5,0.1]]), np.array([[0.2,0.3,0.5],[0.6,0.7,0.2],[0.1,0.4,0.8]]))\n",
    "Biases = (np.array([[0.1],[0.2],[0.3]]), np.array([[0.1],[0.2],[0.3]]))\n",
    "output,preac,postac = source.forward_propagation(input_=A,Weights=Weights,Biases=Biases,activation_sequence=act)\n",
    "opti = source.Optimizer(loss=\"cross_entropy\")\n",
    "gw,gb = opti.backprop_grads(Weights=Weights,Biases=Biases,pre_ac=preac,post_ac=postac,y_true=B,activation_sequence=act)\n",
    "#gw[1].dtype\n",
    "gw, gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.], dtype=float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1 = np.array([1.0,2.0,3.0])\n",
    "\n",
    "A1 = A1.astype(np.longdouble)\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10) (32, 1)\n",
      "(32, 10) (32, 1)\n",
      "(32, 10) (32, 1)\n",
      "(4, 10) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "opti = source.Optimizer()\n",
    "batches = source.batchloader(X_data=np.random.random(size=(100,10)),y_data=np.random.random(size=(100,1)),batch_size=32,shuffle=True)\n",
    "for x,y in batches:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1 = np.array([[1,2],[3,4]])\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A1[:,1],A1[:,1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must be 1- or 2-d.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joeld\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\twodim_base.py:303\u001b[0m, in \u001b[0;36mdiag\u001b[1;34m(v, k)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m diagonal(v, k)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be 1- or 2-d.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Input must be 1- or 2-d."
     ]
    }
   ],
   "source": [
    "np.diag(A1[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 3.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [2., 4.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows, num_columns = A1.shape\n",
    "\n",
    "# Create a 3D array to hold the stacked diagonal matrices\n",
    "stacked_diagonal_matrices = np.zeros((num_rows, num_rows,num_columns,))\n",
    "\n",
    "# Use advanced indexing to fill the diagonals\n",
    "stacked_diagonal_matrices[np.arange(num_rows), np.arange(num_rows), :] = A1.T\n",
    "\n",
    "stacked_diagonal_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (3,5)->(3,5,newaxis) (3,3)->(3,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m input_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m],[\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m10\u001b[39m],[\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m13\u001b[39m,\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m15\u001b[39m]])\n\u001b[0;32m      4\u001b[0m grads \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m],[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m]])\n\u001b[1;32m----> 5\u001b[0m Final_grads \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiff_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joeld\\Desktop\\IITM\\SEM-08\\DA6401 - DEEP LEARNING\\DA6401-A01\\source.py:158\u001b[0m, in \u001b[0;36mdiff_softmax\u001b[1;34m(input_, grads_wrt_postact)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03minput : numpy.ndarray \u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03mReturns :\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03moutput : numpy.ndarray with softmax differentiation actiavtion\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m output_ \u001b[38;5;241m=\u001b[39m softmax(input_)\n\u001b[1;32m--> 158\u001b[0m Diag_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mij,jk->ijk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m Cross_mat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mij,kj->ikj\u001b[39m\u001b[38;5;124m\"\u001b[39m,output_,output_)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Check the dimensions of the matrices\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joeld\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\einsumfunc.py:1371\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[0;32m   1370\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m-> 1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (3,5)->(3,5,newaxis) (3,3)->(3,3) "
     ]
    }
   ],
   "source": [
    "import source\n",
    "import numpy as np\n",
    "input_ = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\n",
    "grads = np.array([[1,1,1,1,1],[2,2,2,2,2],[3,3,3,3,3]])\n",
    "Final_grads = source.diff_softmax(input_,grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mA1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m A1\u001b[38;5;241m.\u001b[39mshape[[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "A1.shape[] == A1.shape[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. ],\n",
       "       [1.5, 2. ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A1 = np.random.random((10,5))\n",
    "A1 = np.array([[1,2],[3,4]])\n",
    "#np.einsum(\"ij,kj->ikj\",A1,A1)[:,:,1]\n",
    "#np.expand_dims(A1,axis=1)\n",
    "b1 = np.array([[1],[2]])\n",
    "\n",
    "A1/b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import source\n",
    "import numpy as np\n",
    "output_ = np.array([[0.1,0.3],[0.5,0.3],[0.4,0.4]])\n",
    "grads = np.array([[0,0],[-1/0.4,0],[0,-1/0.4]])\n",
    "Diag_mat, Cross_mat, Diff_mat, Expanded_grads_postact, Final_grads = source.diff_softmax(output_,grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8, 16],\n",
       "       [32, 64]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.einsum(\"ij,kj->ikj\",A1,A1)*np.expand_dims(np.array([[1,2],[3,4]]),axis=1))[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(np.array([[1,2],[3,4]]),axis=1)[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(A1,axis=2)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 2.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [3., 4.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Diag_mat = np.einsum(\"jk,ij->ijk\", A1, np.eye(A1.shape[0]))[:,:,]\n",
    "Diag_mat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
